{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facial expressions",
      "provenance": [],
      "authorship_tag": "ABX9TyPOhHo/FpRHe43zll90bkU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GunH-colab/CompVis/blob/main/facial_expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G0YpwYsRTr8"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "import random\r\n",
        "import subprocess\r\n",
        "import cv2\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "#We are using keras as our base package to build our classifier. \r\n",
        "import keras\r\n",
        "from keras.layers import *\r\n",
        "from keras.models import *\r\n",
        "from keras.preprocessing import image\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from keras.callbacks import  EarlyStopping\r\n",
        "\r\n",
        "import gc\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "'''\r\n",
        "from keras.layers import Activation\r\n",
        "from keras.utils.generic_utils import get_custom_objects\r\n",
        "'''\r\n",
        "\r\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QksredSp6UZq"
      },
      "source": [
        "def run_cmd(cmd, stderr = subprocess.STDOUT):\r\n",
        "    out = None\r\n",
        "    try:\r\n",
        "        out = subprocess.check_output([cmd], shell=True, stderr = subprocess.STDOUT, universal_newlines = True)\r\n",
        "    except subprocess.CalledProcessError as e:\r\n",
        "        print(f'ERROR {e.returncode}: {cmd}\\n\\t{e.output}', flush=True, file=sys.stderr)\r\n",
        "        raise e\r\n",
        "    return out\r\n",
        "\r\n",
        "def clone_data(data_root):\r\n",
        "    clone_uri = 'https://github.com/muxspace/facial_expressions'\r\n",
        "    if os.path.exists(data_root):\r\n",
        "        assert os.path.isdir(data_root), \\\r\n",
        "        f'{data_root} should be cloned from {clone_uri}'\r\n",
        "    else:\r\n",
        "        print('Cloning the covid facial expression data dataset. It may take a while\\n...\\n', flush=True)\r\n",
        "        run_cmd(f'git clone {clone_uri} {data_root}')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIqdnHiWRRB1"
      },
      "source": [
        "data_root = \"./data\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBAIgMSNRZxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d01fa6-b399-450f-b3ce-6b63c18a109f"
      },
      "source": [
        "clone_data(data_root)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning the covid facial expression data dataset. It may take a while\n",
            "...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rYIv4psRdoc",
        "outputId": "54fa9f20-893e-4911-8b1f-70dc45d56e64"
      },
      "source": [
        "labels = pd.read_csv('/content/data/data/legend.csv')\r\n",
        "labels['target'] = labels.emotion.str.lower()\r\n",
        "labels.drop('emotion', axis = 1, inplace = True)\r\n",
        "\r\n",
        "labels.target.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral      6868\n",
              "happiness    5696\n",
              "surprise      368\n",
              "sadness       268\n",
              "anger         252\n",
              "disgust       208\n",
              "fear           21\n",
              "contempt        9\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIHAc0AOSyOT"
      },
      "source": [
        "import shutil\r\n",
        "\r\n",
        "labels = labels.sort_values('target')\r\n",
        "class_names = list(labels.target.unique())\r\n",
        "\r\n",
        "train_images = '/content/data/images'\r\n",
        "train_cat = '/train_'\r\n",
        "\r\n",
        "for i in class_names:\r\n",
        "    os.makedirs(os.path.join('train_', i))\r\n",
        "\r\n",
        "for c in class_names: # Category Name\r\n",
        "    for i in list(labels[labels['target']==c]['image']): # Image Id\r\n",
        "        get_image = os.path.join('/content/data/images', i) # Path to Images\r\n",
        "        move_image_to_cat = shutil.copy(get_image, 'train_/'+c)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L52SVtqYefTU"
      },
      "source": [
        "def img_gen(train_path_variable):\r\n",
        "  train_preprocess = keras.preprocessing.image.ImageDataGenerator(\r\n",
        "                                        rescale = 1./255,\r\n",
        "                                        zoom_range = 0.2,\r\n",
        "                                        shear_range = 0.2,\r\n",
        "                                        horizontal_flip = True,\r\n",
        "                                        validation_split = 0.2,\r\n",
        "                                    )\r\n",
        "\r\n",
        "  train = train_preprocess.flow_from_directory(\r\n",
        "                                        train_path_variable,\r\n",
        "                                        target_size = (224, 224),\r\n",
        "                                        batch_size = 16,\r\n",
        "                                        class_mode = 'categorical',\r\n",
        "                                        subset = 'training'\r\n",
        "                                    )\r\n",
        "  val = train_preprocess.flow_from_directory(\r\n",
        "                                        train_path_variable,\r\n",
        "                                        target_size = (224, 224),\r\n",
        "                                        batch_size = 16,\r\n",
        "                                        class_mode = 'categorical',\r\n",
        "                                        subset = 'validation'\r\n",
        "                                    )\r\n",
        "  return train, val"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFIJPErvfRBU",
        "outputId": "ff37c82f-d0d1-4004-b54a-5b19590571d0"
      },
      "source": [
        "train, val = img_gen('/content/train_')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10951 images belonging to 8 classes.\n",
            "Found 2732 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFnPEV3fYOM"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu', input_shape = (224, 224, 3)))\r\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu'))\r\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu'))\r\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Conv2D(128, (3, 3), activation = 'relu'))\r\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\r\n",
        "model.add(Dropout(0.25))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(64, activation = 'relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(8, activation = 'softmax'))\r\n",
        "\r\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience = 10)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpAV34eZfrHI",
        "outputId": "6aba3ed4-0ecd-4972-9bdf-94f5260467a4"
      },
      "source": [
        "train.class_indices"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anger': 0,\n",
              " 'contempt': 1,\n",
              " 'disgust': 2,\n",
              " 'fear': 3,\n",
              " 'happiness': 4,\n",
              " 'neutral': 5,\n",
              " 'sadness': 6,\n",
              " 'surprise': 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yZsBY6Af5H1",
        "outputId": "282e5f78-b0e5-46f3-a3d2-6f3f71af8c33"
      },
      "source": [
        "history = model.fit(\r\n",
        "                                train,\r\n",
        "                                validation_data = val,\r\n",
        "                                epochs = 50,\r\n",
        "                                callbacks = [early_stop],\r\n",
        "                                batch_size = 16\r\n",
        "                    )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "685/685 [==============================] - 230s 291ms/step - loss: 1.1753 - accuracy: 0.4630 - val_loss: 0.9731 - val_accuracy: 0.5015\n",
            "Epoch 2/50\n",
            "685/685 [==============================] - 197s 288ms/step - loss: 1.0256 - accuracy: 0.5072 - val_loss: 0.9648 - val_accuracy: 0.5128\n",
            "Epoch 3/50\n",
            "685/685 [==============================] - 196s 286ms/step - loss: 0.9878 - accuracy: 0.5510 - val_loss: 0.8475 - val_accuracy: 0.6680\n",
            "Epoch 4/50\n",
            "685/685 [==============================] - 197s 288ms/step - loss: 0.9202 - accuracy: 0.6334 - val_loss: 0.7974 - val_accuracy: 0.7160\n",
            "Epoch 5/50\n",
            "685/685 [==============================] - 201s 293ms/step - loss: 0.8814 - accuracy: 0.6655 - val_loss: 0.7623 - val_accuracy: 0.7189\n",
            "Epoch 6/50\n",
            "685/685 [==============================] - 200s 292ms/step - loss: 0.8300 - accuracy: 0.7002 - val_loss: 0.7400 - val_accuracy: 0.7233\n",
            "Epoch 7/50\n",
            "685/685 [==============================] - 201s 293ms/step - loss: 0.8203 - accuracy: 0.7040 - val_loss: 0.6688 - val_accuracy: 0.7632\n",
            "Epoch 8/50\n",
            "685/685 [==============================] - 201s 294ms/step - loss: 0.7804 - accuracy: 0.7187 - val_loss: 0.6799 - val_accuracy: 0.7665\n",
            "Epoch 9/50\n",
            "685/685 [==============================] - 201s 294ms/step - loss: 0.7627 - accuracy: 0.7258 - val_loss: 0.6600 - val_accuracy: 0.7570\n",
            "Epoch 10/50\n",
            "685/685 [==============================] - 201s 294ms/step - loss: 0.7281 - accuracy: 0.7471 - val_loss: 0.6673 - val_accuracy: 0.7690\n",
            "Epoch 11/50\n",
            "685/685 [==============================] - 202s 294ms/step - loss: 0.7247 - accuracy: 0.7379 - val_loss: 0.6861 - val_accuracy: 0.7529\n",
            "Epoch 12/50\n",
            "685/685 [==============================] - 202s 294ms/step - loss: 0.7178 - accuracy: 0.7514 - val_loss: 0.6191 - val_accuracy: 0.7694\n",
            "Epoch 13/50\n",
            "685/685 [==============================] - 202s 295ms/step - loss: 0.7123 - accuracy: 0.7496 - val_loss: 0.6444 - val_accuracy: 0.7745\n",
            "Epoch 14/50\n",
            "685/685 [==============================] - 201s 294ms/step - loss: 0.7126 - accuracy: 0.7538 - val_loss: 0.6189 - val_accuracy: 0.7870\n",
            "Epoch 15/50\n",
            "685/685 [==============================] - 202s 294ms/step - loss: 0.6762 - accuracy: 0.7662 - val_loss: 0.6023 - val_accuracy: 0.7829\n",
            "Epoch 16/50\n",
            "685/685 [==============================] - 201s 294ms/step - loss: 0.6774 - accuracy: 0.7606 - val_loss: 0.6347 - val_accuracy: 0.7672\n",
            "Epoch 17/50\n",
            "685/685 [==============================] - 201s 293ms/step - loss: 0.6860 - accuracy: 0.7602 - val_loss: 0.6041 - val_accuracy: 0.7742\n",
            "Epoch 18/50\n",
            "685/685 [==============================] - 200s 292ms/step - loss: 0.6752 - accuracy: 0.7707 - val_loss: 0.5768 - val_accuracy: 0.8020\n",
            "Epoch 19/50\n",
            "685/685 [==============================] - 200s 292ms/step - loss: 0.6427 - accuracy: 0.7802 - val_loss: 0.5853 - val_accuracy: 0.7990\n",
            "Epoch 20/50\n",
            "685/685 [==============================] - 200s 292ms/step - loss: 0.6680 - accuracy: 0.7737 - val_loss: 0.5909 - val_accuracy: 0.7965\n",
            "Epoch 21/50\n",
            "685/685 [==============================] - 200s 291ms/step - loss: 0.6581 - accuracy: 0.7776 - val_loss: 0.5797 - val_accuracy: 0.8023\n",
            "Epoch 22/50\n",
            "685/685 [==============================] - 199s 290ms/step - loss: 0.6736 - accuracy: 0.7666 - val_loss: 0.6029 - val_accuracy: 0.7818\n",
            "Epoch 23/50\n",
            "685/685 [==============================] - 200s 293ms/step - loss: 0.6654 - accuracy: 0.7675 - val_loss: 0.5849 - val_accuracy: 0.7961\n",
            "Epoch 24/50\n",
            "685/685 [==============================] - 200s 292ms/step - loss: 0.6607 - accuracy: 0.7690 - val_loss: 0.6168 - val_accuracy: 0.7844\n",
            "Epoch 25/50\n",
            "685/685 [==============================] - 200s 291ms/step - loss: 0.6480 - accuracy: 0.7742 - val_loss: 0.5926 - val_accuracy: 0.7925\n",
            "Epoch 26/50\n",
            "685/685 [==============================] - 202s 295ms/step - loss: 0.6296 - accuracy: 0.7784 - val_loss: 0.5665 - val_accuracy: 0.8031\n",
            "Epoch 27/50\n",
            "685/685 [==============================] - 203s 297ms/step - loss: 0.6381 - accuracy: 0.7810 - val_loss: 0.5932 - val_accuracy: 0.8016\n",
            "Epoch 28/50\n",
            "685/685 [==============================] - 201s 293ms/step - loss: 0.6383 - accuracy: 0.7823 - val_loss: 0.5704 - val_accuracy: 0.8045\n",
            "Epoch 29/50\n",
            "685/685 [==============================] - 201s 293ms/step - loss: 0.6168 - accuracy: 0.7841 - val_loss: 0.5749 - val_accuracy: 0.8108\n",
            "Epoch 30/50\n",
            "685/685 [==============================] - 198s 289ms/step - loss: 0.6007 - accuracy: 0.7944 - val_loss: 0.5717 - val_accuracy: 0.7983\n",
            "Epoch 31/50\n",
            "685/685 [==============================] - 196s 286ms/step - loss: 0.6214 - accuracy: 0.7896 - val_loss: 0.5689 - val_accuracy: 0.7987\n",
            "Epoch 32/50\n",
            "685/685 [==============================] - 198s 289ms/step - loss: 0.6369 - accuracy: 0.7830 - val_loss: 0.5368 - val_accuracy: 0.8155\n",
            "Epoch 33/50\n",
            "685/685 [==============================] - 196s 286ms/step - loss: 0.6156 - accuracy: 0.7894 - val_loss: 0.5531 - val_accuracy: 0.8086\n",
            "Epoch 34/50\n",
            "685/685 [==============================] - 196s 286ms/step - loss: 0.6126 - accuracy: 0.7896 - val_loss: 0.5413 - val_accuracy: 0.8184\n",
            "Epoch 35/50\n",
            "685/685 [==============================] - 200s 292ms/step - loss: 0.6174 - accuracy: 0.7942 - val_loss: 0.5654 - val_accuracy: 0.8060\n",
            "Epoch 36/50\n",
            "685/685 [==============================] - 201s 293ms/step - loss: 0.6259 - accuracy: 0.7846 - val_loss: 0.5532 - val_accuracy: 0.8082\n",
            "Epoch 37/50\n",
            "685/685 [==============================] - 202s 295ms/step - loss: 0.6022 - accuracy: 0.7840 - val_loss: 0.5375 - val_accuracy: 0.8152\n",
            "Epoch 38/50\n",
            "685/685 [==============================] - 202s 295ms/step - loss: 0.6106 - accuracy: 0.7921 - val_loss: 0.5386 - val_accuracy: 0.8166\n",
            "Epoch 39/50\n",
            "685/685 [==============================] - 203s 296ms/step - loss: 0.5843 - accuracy: 0.7976 - val_loss: 0.5451 - val_accuracy: 0.8104\n",
            "Epoch 40/50\n",
            "685/685 [==============================] - 204s 297ms/step - loss: 0.5957 - accuracy: 0.8015 - val_loss: 0.5383 - val_accuracy: 0.8078\n",
            "Epoch 41/50\n",
            "685/685 [==============================] - 204s 297ms/step - loss: 0.6161 - accuracy: 0.7913 - val_loss: 0.5420 - val_accuracy: 0.8122\n",
            "Epoch 42/50\n",
            "685/685 [==============================] - 204s 297ms/step - loss: 0.6074 - accuracy: 0.7947 - val_loss: 0.5440 - val_accuracy: 0.8170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QacJB7Favok6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}